	这周我主要侧重点是在使用python上，我阅读了show_and_tell的源代码，由于它是使用tensorflow写的，所以我在阅读的时候学习了一些python的API，另外我进行了利用我们制作的数据集来进行训练和测试的实验，实验结果并不好，可能是因为我们的数据集太小的原因，所以我用flickr8k的数据集进行训练，目前正在训练，另外我们小组寒假制作了一些自己的数据集，但是照片数量太少，描述的质量不高，所以为了扩充我们的数据集，我们的想到的解决方案是从flickr8k和flickr30k的数据集中挑选一些可以作为我们数据集的照片和描述，以此来扩充我们的数据集，经过测试和训练，我得到了一些实验数据和参数。
	在提取照片特征制作npy文件时，程序对GPU内存的使用量在1000MB左右，提取flickr8k数据集(8091张图片)的特征耗费了3.5min，提取flickr8k数据集(30713张图片)的特征耗费了15min。接着我们得到特征文件和ckpt文件(show_and_tell的训练模型文件），我又去找到了官方给的预训练模型，并在其的基础上训练模型。我的接下来的计划是看懂源码中是怎么把描述转换成词向量并将照片和词向量扔进lstm网络中进行训练的，另一方面就是vgg16的转换模型也用于test阶段。
	这个星期主要学习了怎么用python进行编写深度学习的代码和数据分析，另外一个小插曲就是我一个同学叫我帮他使用聚类算法对他爬取的全国370个城市的空气质量数据进行分类，找到不同城市之间的相关联系和各个城市的空间污染严重程度。在这个过程中，我学会了一些数据分析的一些基本技巧和源代码使用。并用python编写了knn 和 k-means 的算法。
	最后阐述我们的小组分工和基本计划，我让潘春固和严昊这个星期进行了py-faster-rcnn数据集的制作。并分配任务叫他们去查找选用眼镜是否提供sdk和API接口。另外就是分配黄雪桐咨询中期进度表填写的相关信息和资料整合。
	

	训练速度对比		100张照片		8k照片
			   10 epoches/3 mins           1 epoch/30 mins

	内存对比(32GB)		8k 照片			30k照片
				  25.2			  75











