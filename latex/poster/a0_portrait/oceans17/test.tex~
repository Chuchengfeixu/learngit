
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}
\usepackage{booktabs}       % professional-quality tables
\usepackage{color}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{comment}
\usepackage[fleqn]{amsmath}
\usepackage{multirow}
\usepackage{comment}
% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
%\title{Deep convolutional neural network and data augamentation for fish detection and classifaction}
\title{Fish recognition using deep convolutional neural network and data augmentation}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations

\author{\IEEEauthorblockN{Ziqiang Zheng, Chao Wang, Zhibin Yu*, Haiyong Zheng, Weiwei Wang}
\IEEEauthorblockA{College of Information Science and Engineering\\
Ocean University of China\\
Qingdao 266100, China\\
Email: zhengziqiang1@gmail.com\\
*Corresponding author: yuzhibin@ouc.edu.cn}}


    % conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
%Fish recognition is one of the most significant application of computer vision in fish survey, and the fish classification is crucially required. Different from the best known and the most well-investigated object detection - face detection, fish usually occupy a smaller area in an image than the face of one person. Besides some pictures are taken under water and the pictures are not clear and dark for the light is absorbed by water. For these two reasons, it's a challenge for us to detect and classify the fish. 


Nowadays, as a sub topic of computer vision and fishery industry, fish recognition is still a challenging work not only because of various kinds of fish, but also because of the complex background of images. In this paper, we aim to classify different fish images obtained from cameras of fishing vessels. Fish detection is different with the best known and the most well-investigated object detection - face detection. Fish has more different shapes than human faces. And fish always take only a small part of the whole image. For these two reasons, it's a challenge for us to detect and classify the fish. Our work is done with Kaggle dataset. The Kaggle dataset aims to detect and classify the species of fish. The competition provides a train dataset which contains six species of tuna fish, fish but not tuna and no fish. Eight target categories are available in the dataset. The dataset is critically imbalanced, the Albacore tuna has thousands of images while the Opah has only sixty images. In order to overcome these problems we introduced two methods to improve our accuracy for the fish classification. Our approach can avoid over-fitting caused by the imbalance of training dataset. We employ the AlexNet, GoogLeNet, Caffenet and VGGNet neural network for the classification. As a result of the small dataset, the VGGNet architecture performs worse than AlexNet. We propose a local region based the fish area modeling approach so that the local feature can be modeled. In order to obtain more image information and realize imbalanced datasets classification, we have done some data augmentation. In order to get better performance, we also have done some image preprocessing, and it really works.

\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.

% no keywords



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



% table





\section{Introduction}
% no \IEEEPARstart

%%1.The background of detecting moving object from underwater video. 2.Why we need to do research in detecting moving object from underwater video. 3. Nowadays, people always use what methods to resolve the problem. 4. Shortsages of 5. Our method is what and how to resolve the problem. Compared with others, what advantages.


%Nearly half of the world depends on seafood for their main source of protein. And fish play an important role in our marine ecosystem. So it's valuable for us to do the the fish detection and classification. We research to achieve the fish detection and classification automatically, which has drawn increasing attention.\par
Nearly half the population of the world depends on seafood for their main source of protein. And fish play an important role in our marine ecosystem. So it's valuable for us to do the fish detection and classification. We research to achieve the fish detection and classification automatically\cite{spampinato2010automatic}, which has drawn increasing attention all over the world. And with the fast development of marine resources, the ocean observation is required constantly. The fish recognition is one of the important parts in ocean observation\cite{rodriguez2015fish}.\par

In the western and central pacific, where 60\% of the world's tuna is caught, illegal, unreported, and unregulated fishing practices are threatening marine ecosystems, global seafood supplies and local livelihoods. The Nature Conservancy is inviting the Kaggle community to develop the algorithms to automatically detect and classify species of tunas. The Kaggle dataset includes Albacore tuna, Bigeye tuna, Yellowfin tuna, Mahi, Opah, Shark, Other(meaning that there are fish present but not in the above six categories), and No Fish(meaning that no fish in the picture). We divided the Kaggle dataset into training dataset and validation dataset. And we take four fifths of the images provided by Kaggle as the training dataset and take the rest as the validation dataset. Each image has only one fish category. And the Table~\ref{table:table1} shows the imbalance of the Kaggle dataset. From this table we can see that the top three categories of fish images almost account for four fifths of the dataset.\par
\begin{table}[t]
  \caption{Kaggle database}
  \label{table:table1}
  \centering
  \begin{tabular}{llll}
    \toprule
    \cmidrule{1-4}
    classes     & Total     & Training   &Valiation \\
    \hline
    ALB & 45.5\%  & 43.1\%  & 47.5\%     \\
    YFT     & 19.4\% & 20.6\%   &21.0\%    \\
    NoF     & 12.3\%   & 13.8\% &13.3\% \\
    OTHER & 7.9\% & 7.6\% & 8.1\% \\
    BET & 5.2\% & 5.4\% & 4.2\% \\ 
    \toprule 
    Sum & 90.3\% & 90.5\% & 94.0\% \\  
    \bottomrule
  \end{tabular}
\end{table}

%process the data
\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{figures/process.jpg}
\hspace{0.05in}
\caption{First we make a mask in the fish region, then we rotate the processed images at different angles and get some new different images. After the data augmentation, at last we use the CNN to fish the fish classification.}
\label{fig:process}
\end{center}
\end{figure}
Most prior recognition researches are based on the object on the ground such as face detection, car detection and pedestrian detection\cite{anantharajah2014local}. Fish detection is different from recognition researches on the ground\cite{socher2012convolutional}. For the reason that the fish images are usually uncleared and the fish only cover small areas in an image, the fish detection is more difficult than the object detection on the ground. And the shape of the fish is more complex than face and cars. So the fish recognition needs more sophisticated detection and requires the methods to locate where the fish are in the fish images\cite{huang2012underwater}. It's difficult for our machines to distinguish the fish and the background. Even some machine leaning methods can learn some high-level features from the fish images, they can't make sure that the extracted features are that really make contribution to the recognition.  Another challenge for fish recognition is that the images from the cameras of fishing vessels are uncleared. It's difficult for our model to learn the characteristics of the object by the unclear pictures\cite{rodrigues2010automatic}. Some image processing methods could be employed to solve the problem.\cite{lu2013underwater} We have used some image processing methods in our experiments and it really works. Recently, several papers have brought some methods for fish detection and classification\cite{chuang2016feature}. We have employed the deep convolutional neural network for the research because of its fantastic performance in image detection and classification. It is an efficient classification method for images and widely used in object recognition tasks\cite{qin2015deepfish}.\par
% We train our model with both the raw images and the processed images. This method can force our model concentrate on the fish region by comparing these two different images.
In this paper, we will introduce two methods to improve the classification accuracy. Our methods can be described with the Fig.~\ref{fig:process}. We have done some data preprocessing before training our model. First we make a mask in the fish region and make the fish region black, which we regard as the NoF fish images. Then we use some methods to achieve data augmentation. At last we use the convolutional neural network(CNN) as the classifier. The first method is based on data augmentation. In consideration of the imbalance of the dataset, we got some new images by rotating the selected images. Through this method, we can increase the number of some species of fish images, which can help avoid that our model is over-fitted with some categories of fish images, and rotating the fish images can improve the robustness of detection and achieve sophisticated detection. We increased the size of our datasets in this way and the validation accuracy increased two percent.  \par



%data distribution
\begin{figure}[!ht]
  
\centering
\subfigure[Training database]{
  \label{fig:a}
  \includegraphics[width=1\linewidth]{figures/train_data.png}}
  \hspace{0.05in}
\subfigure[Valing database]{
  \label{fig:b}
  \includegraphics[width=1\linewidth]{figures/val_data.png}}
  \hspace{0.05in}
  \caption{Data distribution of Kaggle}
  \label{fig:data}

\end{figure}



The second improvement is based on an image preprocessing method. In order to force the neural networks to focus on fish not the vessel, we have made a mask of the area where the fish are located, and we have used both the processed images and the raw images to train our model. Fig.~\ref{fig:mosaic} shows the differences between the two kinds of images. This method can force our neural network to concentrate on the area where the fish present. And the features are learned from comparing the raw images and the images with mask. In order to prove our assumption, we test some fish images and get the image data from the last pooling layer. The higher value area in the pooling layer means higher contribution to the final decision. Through comparing the pooling gray image with the raw image, we found that the area of fish is brighter than other areas, which proves what we expected that the fish area make greater contribution in the classification. Fig.~\ref{fig:pool} illustrates the image information of the region of fish and other regions without fish. We can see that the region of fish is brighter than other regions and the pooling image data can confirm that the learned features are from fish. In addition, we use the processed images to fine-tune our pre-trained model and it performs better when testing.\par

The architecture of our model used in the experiment is the convolutional neural network, which has achieved remarkable performance in image classification and object detection recently\cite{sharif2014cnn}\cite{krizhevsky2012imagenet}. And CNN is usually composed of convolutional layer, pooling layer and fully connected layer. Compared with feature-designed extraction, it can explore more abstract and high-level information by deep neural network\cite{lee2009convolutional}. Convolution neural networks have been applied with great success to the image recognition of objects on the ground. Anologously we can apply it to the fish recognition. We have employed different CNN architectures for the fish classification. But there still exists the problem that our model can not distinguish between foreground and background. Our methods can prevent the CNN architecture model from concentrating on the boat region rather than the fish region. In addition, we use the processed images to fine-tune our pre-trained model and it performs better when testing. This method can help avoid over-fitting the of our model.\par

% mosaic pictures
\begin{figure}[!ht]
\centering

\subfigure[the raw image]{
  \label{fig:a}
  \includegraphics[width=1\linewidth]{figures/testout.png}}
  %\hspace{0.15in}
%\end{figure}
%\begin{figure}[!ht]
\subfigure[image with mask]{
  \label{fig:b}
  \includegraphics[width=1\linewidth]{figures/mask_raw.jpg}}
  %\hspace{0.15in}
  
  \caption{We make a mask of the area where the fish are located, because the mask can force our model to concentrate on the fish area by comparing the raw images and the processed images. So it can help our convolutional neural network detect the fish region. we have trained our model using both the two images at the same time, which can improve the robustness of the fish recognition.}
   \label{fig:mosaic}

\end{figure}


%Recently, several papers have brought some methods for fish detection and classification\cite{chuang2016feature}. Spatial constraints are are introduced by dividing the image to regions and learning the feature region\cite{anantharajah2014local}. In order to divide the image to feature regions and background regions, We have made a mask in the fish region. And we use the mask images to lead our model to fit the feature regions and ignore the other regions.\cite{ge2015modelling} 

%the accuracy using rotating
%\par
%The architecture of our model used in the experiment is the convolutional neural network(CNN). And CNN is usually composed of convolutional layer, pooling layer and fully connected layer. CNN has achieved remarkable performance in image classification recently. Compared with feature-designed extraction, it can explore more abstract and high-level information by deep neural network. So we can apply it in fish classification. 
%\hfill mds
%\hfill January 11, 2007
%\section{Transfer reinforce model and Training}
%Fig.~\ref{fig:network} illustrates the final transfer reinforce model archicture. We take the whole data as the input of our model and fixed model A as a feature extracter as expected to reinforce the small class's features. 

%\subsection{Archicture}


%\begin{figure*}[!ht]
%\centering
%\includegraphics[width=0.85\textwidth]{figures/network.png}
%\caption{Model structure of our plankton classifaction neural network.}
%\label{fig:network}
%\end{figure*}

%\subsection{Training}


%\section{Experiments}
%Fig.~\ref{fig:a}, Fig.~\ref{fig:b}


% table
\begin{table}[t]
  \caption{singal model result on full data}
  \label{table:table2}
  \centering
  \begin{tabular}{lll}
    \toprule
    \cmidrule{1-3}
    database     &model       &accuracy \\
    \midrule
    full dataset & alexnet     & 0.94 \\
    full dataset & googlenet    &0.942 \\
    full dataset & vgg16    & 0.44 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[t]
\small
\centering
\caption{The test loss in the Kaggle competition}
\label{table:table3}
\begin{tabular}{lll}
    \toprule
    \cmidrule{1-3}
     method     &model      &test loss \\
	\midrule
	None	& Caffenet	& 1.92 \\
       None	& GoogleNet	& 2.56 \\
       Rotating & Caffenet     & 1.77  \\
       Rotating & GoogleNet   & 1.93    \\
	Mask	& Caffenet	& 1.87   \\
	Mask	& GoogleNet	& 2.25    \\
	Rotating + Mask &Caffenet & 1.71   \\
	Rotating + Mask &GoogleNet & 1.85  \\
       \bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\small
\centering
\caption{Kaggle database}
\label{table:table4}
\begin{tabular}{lll}
    \toprule
    \cmidrule{1-3}
     database     &model      &accuracy \\
	\midrule
       full+rotating & alexnet     & 0.9673  \\
       full+rotating & googlenet   &0.964    \\
       full+rotating  & vgg16  & 0.45  \\
       \bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\small
\centering
\caption{Kaggle database}
\label{table:table5}
\begin{tabular}{lll}
    \toprule
    \cmidrule{1-3}
     database     &model        &accuracy \\
	\midrule
       full+rotating+mask & alexnet     & 0.9703  \\
       full+rotating+mask & googlenet   &0.954    \\
  
       \bottomrule
\end{tabular}
\end{table}
\section{Experiments}
The Table~\ref{table:table1} indicates the details of the Kaggle Database. The first column is the name of fish and the numbers in the rest columns mean that the proportion of this class in all fish. And the Fig.~\ref{fig:data} is also the another form of fish proportion. We can see the distribution of the dataset is imbalanced. The ALB and YFT fish images cover nearly two thirds of the image dataset while the LAG fish category only account for a few percent. The imbalance of the dataset can lead to the over-fitting of model, and our model can't extract enough features from those fish categories that account for only a small part of the dataset. So it's a challenge for our model to realize the feature learning on the imbalanced dataset. In order to solve this problem, we rotate some selected fish images to get new fish images. We got eight times images by rotating the fish images at different angles. We have made the data augmentation by this method, which can help to solve the imbalance of the dataset.\par



The Table~\ref{table:table2} illustrates the benchmarks. The Table~\ref{table:table3} shows the test loss of our models uploaded in the Kaggle competition. We have trained the dataset with four different deep convolutional neural network structure, Alexnet, GoogleNet, Caffenet and VGGNet\cite{ge2015modelling}. Above the accuracy table, we can see that the AlexNet and the GoogLeNet achieved greater performance than the VGGNet. Due to the imbalance and the insufficience of the dataset, the VGGNet could be over-fitted with our training set so that it performs badly when testing. And the VGGNet wastes more time to converge on small dataset than GoogleNet and Alexnet. Comparing the accuracy between Alexnet and GoogleNet, we come to the conclusion that the GoogleNet performs better than Alexnet because of more layers and parameters. The Table~\ref{table:table4} shows the accuracy that we have rotated some images to get new images. From the table we can see this method actually worked out and the accuracy has increased about two percent. And by comparing the loss we can see that the loss has decreased by using the rotating method. According to this, the data augmentation method can help our model extract high-level features from the fish images. And our model can be more robust in this way. Basing this, we have made a mask of the region of fish for some fish images. In this experiment, we have picked up about one fourth of the seven fish category images except for the NoF images, we made the mask int the images and replace the region where the fish are with the black areas. Then we put these processed images into the NoF category. The Fig.~\ref{fig:mosaic} shows the differences between the raw image and the image with mask. Then We trained our model with the raw images and the processed images using different architecture of networks. By comparing these different images, our model can concentrate on the region of the fish and learn more features of fish rather than the feature of the background. The accuracy has also increased in the Table~\ref{table:table5}, which proves that our methods are effective. By comparing the test loss int the Table~\ref{table:table3}, we can see that both the rotating method and the mask method can decrease the test loss. The test loss when using both the two methods at the same time has decreased more than only using one method. And because we make a mask in the fish region, our model can concentrate on the fish area quickly. So the mask method can accelerates the convergence rate. \par
In order to prove that the fish region has really made contribution in the fish recognition, we modified Caffe\cite{jia2014caffe} to get the pooling images. And the Fig.~\ref{fig:pool} shows the comparison between the raw images and the pooling images. In consideration of the big number of the pooling images, we have picked up 36 pooling images from 64 pooling images and resized each of all to merge them into one image. From the two sets of images, we can see that the fish region is brighter than other regions in most of the pooling images, and this proves that fish make more contribution than the background for the classification.\par
%pooling image
\begin{figure}[!ht]
\centering
\subfigure[the raw image]{
  \label{fig:a}
  \includegraphics[width=1\linewidth]{figures/resize.jpg}}
  \hspace{0.15in}
%\end{figure}
%\begin{figure}[!ht]
\subfigure[the pooling image]{
  \label{fig:b}
  \includegraphics[width=1\linewidth]{figures/merge_2.jpg}}
  \hspace{0.15in}
  \caption{There are 36 pooling images in the final pooling image. Considering that the size of the pooling images and the number of them, we make them smaller by resizing the images and merge them into one image. By comparing the two images, The region of fish are brighter than other regions in the most of the pooling images, which can prove that the fish has more influence on the classification than other regions.}
  \label{fig:pool}
\end{figure}

\section{Conclusion}

%We have used two methods to help our model to detect the fish and avoid the over-fitting. We have done the data augmentation and found that it can improve the accuracy and help avoid over-fitting. And comparing the raw image and the mask image can help our model to detect the fish. We have used both the two methods simultaneously and the classification accuracy has increased more than two percent.
In this paper, we used two methods to improve the deep neural networks to detect the fish more efficiently. The experiment results show that our methods can classify fish from images captured on a camera of fishing vessels well. And the classification accuracy has increased more than two percent while the test loss decreased. The data augmentation method can solve the imbalance of the dataset well and it can also improve the accuracy of our model. The image processing methods such as rotating and making mask can also work to get the better performance. These methods could be used for sophisticated detection and recognition, which can help to get more high-level and abstract features from images. Our next research is to merge the different networks and make good use of them to get a better performance. Our research can benefit the develop of the marine resources, as well as commercial applications such as fisheries and aquaculture.

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\end{thebibliography}
\bibliographystyle{IEEEtran}
\bibliography{test}


% that's all folks
\end{document}


