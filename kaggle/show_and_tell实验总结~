	show_and_tell的照片描述实验我已经实现了，我也懂得了怎么制作自己的数据集，我已经叫我的队友自己去制作数据集，叫他们根据盲人的生活环境制作数据集，图片标注是一个很大的工作量，flickr数据集的标注是有5个级别的标注，按层次详细程度不断下降。我们要制作数据集也需要按照这样的数据格式。	
	目前我做的工作是show and tell，我将代码和照片都跑通了，但是我还没有实现一张张分帧图片的实验，tf16.model的转化我还没有实现，按照网上给的教程，我使用tensorflow-vgg16将vgg_bvlc_ilsvrc.caffemodel进行转化，但是在转化的时候总是会报一个错误，a cuda runtime call was likely performed without using ....
	我在stack overflow找到了这个问题的帖子，发现大多数出现这个问题的是用theano跑的实验,而我是用的tensorflow,有的人猜测是GPU出了问题，建议关闭GPU通道，这个目前我还没有尝试过，但是我觉得问题可能没有那么简单，很可能是GPU tensorflow的配置出了问题。
	视频特征提取我在网上也找到了代码，我使用VGGFeatExtract来提取特征，在提取特征的时候该源代码默认提取的是fc6和fc7层的代码，如果要提取fc7的代码需要加上--layers fc7,在运行该代码的时候会报一些错误，是因为python中一个函数format的问题，需要我们更改源代码，具体更改方法我在VGGFeatExtract目录下的my_video_demo.py中注释了。另外该源代码提取出来的特征是以mat格式进行存储的，如果要进行视频描述的实验，我采用的源代码是网上挂出的sv2t的源码，下面是他们的项目主页，https://vsubhashini.github.io/s2vt.html，但是在他们的项目主页中并没有挂出怎么进行视频特征提取和视频标注的方法，所以这是一个很大的挑战，我需要把它的源码完全看懂然后才能根据自己的需要更改源码，目前我完成的工作是将生成的demo.mat拿进网络去训练，但是在训练的时候出现了很多问题，我用的提特征的方法和它的不同，所以在扔进网络巡训练的时候报了许多错误，我正在尝试看懂源码及更改源码，目前我更改的源码是framefc7_stream_mat_text_to_hdf5_data.py文件，在这个文件中，我将feature map去掉，我认为这是一个解决方案，目前正在尝试，另一方面的工作是视频标注，我在网上找到了一个视频标注的工具，下面是网页链接，http://web.mit.edu/vondrick/vatic/，这个工具解决了怎么实现视频标注，另外在sv2t的项目主页中我得知他们使用的是M-VAD数据集，我在网上找到了这个数据集的文件，但是还有很多不懂的地方，上面提供的数据集根本不能拿去训练，所以我要继续查阅相关资料，另外google提供了一个视频描述的接口，可以通过其进行视频标注。
																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																														
